{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423bb0d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# KHDLUD Team 10\n",
    "\n",
    "Thành viên:<br>\n",
    "Trần Quốc Long - 18120202\n",
    "<br>Nguyễn Huy Danh - 1712318\n",
    "<br>Trần Đức Anh - 18120280\n",
    "<br>Du Chí Nhân - 18120492"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd808578",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cc98f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc, os\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1830b23f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Đường dẫn\n",
    "test_original_data  = '../input/test.csv'\n",
    "train_original_data = '../input/train.csv'\n",
    "sample_submission_file = '../input/sample_submission.csv'\n",
    "SUBMIT_FILE_PATH = f'../output/2nd-place-solution.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f22828",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ddd97",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Link cuộc thi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e913e8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "kaggle competion:\n",
    "https://www.kaggle.com/c/santander-customer-transaction-prediction#\n",
    "    \n",
    "solution git:\n",
    "https://github.com/KazukiOnodera/Santander-Customer-Transaction-Prediction/blob/master/final_solution/akiyama/py/lgb_train_and_predict.py\n",
    "\n",
    "golf src:\n",
    "https://github.com/KazukiOnodera/santander-customer-transaction-prediction/blob/master/py/990_2nd_place_solution_golf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90eadf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Giới thiệu chủ đề: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b29775",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tại Santander, sứ mệnh của chúng tôi là giúp mọi người và doanh nghiệp phát triển thịnh vượng. Chúng tôi luôn tìm cách giúp khách hàng hiểu được sức khỏe tài chính của họ và xác định những sản phẩm và dịch vụ nào có thể giúp họ đạt được các mục tiêu về tiền tệ của mình.\n",
    "\n",
    "Nhóm khoa học dữ liệu của chúng tôi liên tục thử thách các thuật toán học máy và làm việc với cộng đồng khoa học dữ liệu toàn cầu để đảm bảo chúng tôi có thể xác định chính xác hơn các cách mới để giải quyết thách thức phổ biến nhất, các vấn đề phân loại nhị phân, chẳng hạn như:\n",
    "\n",
    "Khách hàng có hài lòng không? \n",
    "Một khách hàng sẽ mua sản phẩm này? \n",
    "Khách hàng có thể trả khoản vay này không?\n",
    "\n",
    "Trong thử thách này, chúng tôi mời Kagglers giúp chúng tôi xác định khách hàng nào sẽ thực hiện một giao dịch cụ thể trong tương lai, bất kể số tiền đã giao dịch. Dữ liệu được cung cấp cho cuộc thi này có cấu trúc giống với dữ liệu thực mà chúng tôi có sẵn để giải quyết vấn đề này."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7e37b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tập dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d967f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_original_data)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af22f6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tập dữ liệu kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a0f8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(test_original_data)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ba1b7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cách tính điểm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da607342",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Kết quả sẽ được đánh giá dựa trên đối tượng đó có khả năng sẽ thanh toán trong tương lại hay không. Điểm số sẽ được đánh giá bằng độ đo khu vực dưới đường cong ROC giữa xác suất xảy ra và kết quả mục tiêu do sự bất cân đối giữa 2 giá trị 0 và 1 ( Không và có)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f50ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = train['target'].value_counts()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d99e2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2. Các bước thực hiện"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad882a14",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Loại bỏ những giá trị giả"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05715a0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Link: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n",
    "\n",
    "Input: Dữ liệu của tập test\n",
    "\n",
    "Output: Dữ liệu của tập test đã được loại bỏ dữ liệu giả (Fake Samples)\n",
    "\n",
    "Mục đích: Sau khi kiểm tra tập test và tập train thì một điều bất cập đã được nhận ra là: Trong trong tập test có các giá trị độc nhất ở trên từng tính trạng khác với tập train. Tác giả đã đoán rằng người ta đã sử dụng kĩ thuật sampling distributions (phân phối mẫu) từ dữ liệu mẫu thật trong thống kê để tạo ra một số mẫu tổng hợp trong dữ liệu test. Và chúng ta sẽ lọc đi những mẫu dữ liệu giả ấy vì những mẫu giả ấy sẽ không đóng vai trò tính điểm trong tập test chính thức.\n",
    "\n",
    "Quy trình thực hiện: Dựa trên kiến thức về phân phối mẫu, tác giả đã nghĩ ra một cách để phân loại các dữ liệu giả đấy khi kiểm tra đó chính là<br>\n",
    "+Nếu 1 mẫu có ít nhất 1 tính trạng mang giá trị độc nhất thì đó sẽ là mẫu thật<br>\n",
    "+Nếu 1 mẫu sau khi kiểm tra toàn bộ đều không có giá trị độc nhất thì đó chính là mẫu giả.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc1e283",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 43.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.53 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "te_ = pd.read_csv(test_original_data).drop(['ID_code'], axis=1).values\n",
    "\n",
    "unique_samples = []\n",
    "unique_count = np.zeros_like(te_)\n",
    "for feature in tqdm(range(te_.shape[1])):\n",
    "    _, index_, count_ = np.unique(te_[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index_[count_ == 1], feature] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9018c0eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5555851",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Ghép tập train và tập test sau đó nghịch đảo một số tính trạng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c65889",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Link: https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals\n",
    "\n",
    "Input: Dữ liệu của tập test và tập train\n",
    "\n",
    "Output: Dữ liệu dùng để xử sau khi kết hợp và nghịch đảo\n",
    "\n",
    "Mục đích: Sau khi kiểm tra xác suất xảy ra bằng phân phối bayes chúng ta thấy được có 2 loại biểu đồ là biểu đồ có xác suất P lệch về bên trái và lệch về bên phải. Chúng ta sẽ nghịch đảo 1 loại biểu đồ lại để cho các điểm tương đồng trở nên rõ ràng hơn.\n",
    "\n",
    "Quy trình thực hiện: Chúng ta sẽ tìm ở trên tập biểu đồ những biểu đồ có xác suất lệch về bên phải và nghịch đảo chúng để xác suất của chúng đồng nhất về bên trái."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deab97b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before:\n",
    "    \n",
    "![title](img/var_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b100fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After\n",
    "\n",
    "![Title](img/var_after.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df96770",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv(train_original_data)\n",
    "test  = pd.read_csv(test_original_data).drop(synthetic_samples_indexes)\n",
    "\n",
    "X_train = train.iloc[:, 2:].values\n",
    "y_train = train.target.values\n",
    "\n",
    "X_test = test.iloc[:, 1:].values\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "del X_train, X_test; gc.collect()\n",
    "\n",
    "reverse_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26,\n",
    "                27, 29, 32, 35, 37, 40, 41, 47, 48, 49, 51, 52, 53, 55, 60, 61,\n",
    "                62, 65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94,\n",
    "                95, 96, 97, 99, 103, 105, 106, 110, 111, 112, 118, 119, 125, 128,\n",
    "                130, 133, 134, 135, 137, 138, 140, 144, 145, 147, 151, 155, 157,\n",
    "                159, 161, 162, 163, 164, 167, 168, 170, 171, 173, 175, 176, 179,\n",
    "                180, 181, 184, 185, 187, 189, 190, 191, 195, 196, 199,\n",
    "                \n",
    "                ]\n",
    "\n",
    "for j in reverse_list:\n",
    "    X[:, j] *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cf113",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3. Loại bỏ một số tính trạng không cần thiết và Standrad Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e90e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Dữ liệu sau khi đã loại bỏ fake và reverse\n",
    "\n",
    "Output: Dữ liệu đã chuẩn hóa\n",
    "\n",
    "Mục đích: Sau khi đã loại bỏ các mẫu fake và những giá trị gây nhiễu, chuẩn hóa dữ liệu sẽ giúp dễ dàng nhận ra các điểm dữ liệu bất thường. Ngoài ra, còn giúp tránh được những vấn đề do  sự khác biệt về độ đo, phục vụ cho việc unpivot ở bước 6\n",
    "\n",
    "Quy trình thực hiện: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b616819",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drop_vars = [7,\n",
    "            10,\n",
    "            17,\n",
    "            27,\n",
    "            29,\n",
    "            30,\n",
    "            38,\n",
    "            41,\n",
    "            46,\n",
    "            96,\n",
    "            100,\n",
    "            103,\n",
    "            126,\n",
    "            158,\n",
    "            185]\n",
    "\n",
    "var_len = 200 - len(drop_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d21ad09",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 900 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# drop\n",
    "X = np.delete(X, drop_vars, 1)\n",
    "\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40d24e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4. Count round Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd24062",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Output của bước 3\n",
    "\n",
    "Output: Dữ liệu cũ + các cột đếm số lượng theo từng cột\n",
    "\n",
    "Mục đích: Feature engineering, giúp cải thiện kết quả\n",
    "\n",
    "Quy trình thực hiện:<br>\n",
    "Count encoding - Với mỗi cột, đếm số lượng giá trị và tạo ra cột mới bằng cách ánh xạ: giá trị -> số lượng<br>\n",
    "Count round encoding - Tương tự như 4 nhưng giá trị được làm tròn lần lượt là 1,2 và 3 chữ số thập phân rồi mới đếm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd53738",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:41<00:00,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# count encoding\n",
    "X_cnt = np.zeros((len(X), var_len * 4))\n",
    "\n",
    "for j in tqdm(range(var_len)):\n",
    "    for i in range(1, 4):\n",
    "        x = np.round(X[:, j], i+1)\n",
    "        dic = pd.value_counts(x).to_dict()\n",
    "        X_cnt[:, i+j*4] = pd.Series(x).map(dic)\n",
    "    x = X[:, j]\n",
    "    dic = pd.value_counts(x).to_dict()\n",
    "    X_cnt[:, j*4] = pd.Series(x).map(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db525b8d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5. Unpivot tất cả các cột"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b8d7cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Output của bước 4\n",
    "\n",
    "Output: Họ cho rằng giá trị các cột là độc lập với nhau và có thể dùng giá trị của 1 cột để dự đoán target nên họ unpivot\n",
    "\n",
    "Mục đích: Hỗ trợ dự đoán target vì họ đã thử và thấy có cải thiện kết quả\n",
    "\n",
    "Thực hiện: Cứ mỗi biến sẽ là 6 cột (Raw_value, 4 cột đã encode, thuộc về cột nào) đem đi stack vào nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4f7609",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [00:02<00:00, 71.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# raw + count feature\n",
    "X_raw = X.copy() # rename for readable\n",
    "del X; gc.collect()\n",
    "\n",
    "X = np.zeros((len(X_raw), var_len * 5))\n",
    "for j in tqdm(range(var_len)):\n",
    "    X[:, 5*j+1:5*j+5] = X_cnt[:, 4*j:4*j+4]\n",
    "    X[:, 5*j] = X_raw[:, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da799f2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# treat each var as same\n",
    "X_train_concat = np.concatenate([\n",
    "    np.concatenate([\n",
    "        X[:200000, 5*cnum:5*cnum+5], \n",
    "        np.ones((len(y_train), 1)).astype(\"int\")*cnum\n",
    "    ], axis=1) for cnum in range(var_len)], axis=0)\n",
    "y_train_concat = np.concatenate([y_train for cnum in range(var_len)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d0552",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Và đây là kết quả sau khi tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545bf2e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw values</th>\n",
       "      <th>count_1</th>\n",
       "      <th>count_2</th>\n",
       "      <th>count_3</th>\n",
       "      <th>count_4</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.575796</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1093.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.271663</td>\n",
       "      <td>3.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.679857</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1015.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.126794</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1084.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.275857</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999995</th>\n",
       "      <td>1.396420</td>\n",
       "      <td>2.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999996</th>\n",
       "      <td>-0.348698</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999997</th>\n",
       "      <td>-0.718784</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999998</th>\n",
       "      <td>1.016240</td>\n",
       "      <td>1.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999999</th>\n",
       "      <td>-0.329497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>993.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37000000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          raw values  count_1  count_2  count_3  count_4    var\n",
       "0           0.575796      8.0   1093.0    122.0     14.0    0.0\n",
       "1          -0.271663      3.0    973.0     87.0      6.0    0.0\n",
       "2           0.679857      6.0   1015.0     94.0     14.0    0.0\n",
       "3          -0.126794      3.0   1084.0     93.0      6.0    0.0\n",
       "4           0.275857      8.0   1131.0    135.0     18.0    0.0\n",
       "...              ...      ...      ...      ...      ...    ...\n",
       "36999995    1.396420      2.0    527.0     39.0      6.0  184.0\n",
       "36999996   -0.348698      2.0   1017.0    113.0     13.0  184.0\n",
       "36999997   -0.718784      1.0   1101.0    116.0     17.0  184.0\n",
       "36999998    1.016240      1.0    800.0     68.0      6.0  184.0\n",
       "36999999   -0.329497      1.0    993.0     83.0      5.0  184.0\n",
       "\n",
       "[37000000 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show = pd.DataFrame(X_train_concat, columns = ['raw values', 'count_1', 'count_2', 'count_3', 'count_4','var'])\n",
    "show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bdbcc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6. Training và Predict (Continue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f812845b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: dữ liệu sau bước unpivot được gán tên các cột vào\n",
    "\n",
    "Output: các model sau khi được train\n",
    "\n",
    "Mục đích: từ data sau khi đã chuẩn hóa, tạo ra các model, dùng các model này để dự đoán\n",
    "\n",
    "Mô tả: Trong solution, tác giả train và predict sử dụng 2 phương pháp, là Neural Network với thư viện keras và Gradient Boosting với framework LightBGM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27998e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbc219",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a556c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ModelExtractionCallback(object):\n",
    "    \"\"\"\n",
    "    original author : momijiame\n",
    "    ref : https://blog.amedama.jp/entry/lightgbm-cv-model\n",
    "    description : Class for callback to extract trained models from lightgbm.cv(). \n",
    "    note: This class depends on private class '_CVBooster', so there are some future risks. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def __call__(self, env):\n",
    "        # _CVBooster の参照を保持する\n",
    "        self._model = env.model\n",
    "\n",
    "    def _assert_called_cb(self):\n",
    "        if self._model is None:\n",
    "            # コールバックが呼ばれていないときは例外にする\n",
    "            raise RuntimeError('callback has not called yet')\n",
    "\n",
    "    @property\n",
    "    def boosters_proxy(self):\n",
    "        self._assert_called_cb()\n",
    "        # Booster へのプロキシオブジェクトを返す\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def raw_boosters(self):\n",
    "        self._assert_called_cb()\n",
    "        # Booster のリストを返す\n",
    "        return self._model.boosters\n",
    "\n",
    "    @property\n",
    "    def best_iteration(self):\n",
    "        self._assert_called_cb()\n",
    "        # Early stop したときの boosting round を返す\n",
    "        return self._model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9487111a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_output_dir = f'../processed/lgb_output/'\n",
    "if not os.path.isdir(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "dataset_dir = '../processed/dataset/'\n",
    "X_train = pd.read_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "X_test = pd.read_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "\n",
    "params = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 1.0,\n",
    "    'learning_rate': 0.005,\n",
    "    'max_depth': -1,\n",
    "    'metric':'binary_logloss',\n",
    "    'min_data_in_leaf': 30,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 64,\n",
    "    'num_threads': cpu_count(),\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': 1}\n",
    "\n",
    "train_dset = lgb.Dataset(\n",
    "    concat_X_train, \n",
    "    pd.concat([y_train for c in range(200)], axis=0), \n",
    "    free_raw_data=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac7ccb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('CPU train: ' + cpuinfo.get_cpu_info()['brand_raw'] + ' ' + cpuinfo.get_cpu_info()['hz_advertised_friendly'])\n",
    "for fold_set_number in range(10):\n",
    "    print('### start iter {} in 10 ###'.format(fold_set_number+1))\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019+fold_set_number)\n",
    "    folds = [\n",
    "        [\n",
    "            np.concatenate([_trn+i * X_train.shape[0] for i in range(200)]), \n",
    "            np.concatenate([_val+i * X_train.shape[0] for i in range(200)])\n",
    "        ] for _trn, _val in skf.split(X_train, y_train)]\n",
    "\n",
    "    extraction_cb = ModelExtractionCallback()\n",
    "    callbacks = [extraction_cb,]\n",
    "\n",
    "    print('start training. ')\n",
    "    cv_result = lgb.cv(params, train_set=train_dset, num_boost_round=100000, \n",
    "                             early_stopping_rounds=100, verbose_eval=100, folds=folds, callbacks=callbacks)\n",
    "    bsts = extraction_cb.raw_boosters\n",
    "    best_iteration = extraction_cb.best_iteration\n",
    "    print('training end. ')\n",
    "\n",
    "    print('start predicting. ')\n",
    "    oof_pred_array = np.ones((X_train.shape[0], 200))\n",
    "    test_pred_array = np.ones((X_test.shape[0], 5, 200))\n",
    "    for cnum in tqdm(range(200)):\n",
    "        for i, bst in enumerate(bsts):\n",
    "            cv_valid_index = bst.valid_sets[0].used_indices\n",
    "            cv_valid_index = cv_valid_index[:int(cv_valid_index.shape[0]/200)]\n",
    "            # oofの予測\n",
    "            cv_valid_data = arrange_dataset(X_train, cnum).iloc[cv_valid_index].values\n",
    "            oof_pred_array[cv_valid_index, cnum] = bst.predict(cv_valid_data, num_iteration=best_iteration)\n",
    "            # testの予測\n",
    "            test_pred_array[:, i, cnum] = bst.predict(arrange_dataset(X_test, cnum).values, num_iteration=best_iteration)\n",
    "    print('prediction end. ')\n",
    "\n",
    "    print('start postprocess. ')\n",
    "    thr = 0.500\n",
    "    oof_pred_odds_prod = np.ones((X_train.shape[0]))\n",
    "    test_pred_odds_prod = np.ones((X_test.shape[0], 5))\n",
    "    for cnum in tqdm(range(200)):\n",
    "        tmp_auc = roc_auc_score(y_train, oof_pred_array[:, cnum])\n",
    "        if tmp_auc >= thr:\n",
    "            oof_pred_odds_prod *= oof_pred_array[:, cnum] / (1 - oof_pred_array[:, cnum])\n",
    "            test_pred_odds_prod *= test_pred_array[:,:, cnum] / (1 - test_pred_array[:,:, cnum])\n",
    "    print('postprocess end. auc : {0:.6f}'.format(roc_auc_score(y_train, oof_pred_odds_prod)))\n",
    "\n",
    "    print('save iteration results')\n",
    "    pd.DataFrame(oof_pred_odds_prod, index=X_train.index, columns=['pred'])\\\n",
    "        .to_pickle(os.path.join(model_output_dir, f'oof_preds_{fold_set_number}.pkl.gz'), compression='gzip')\n",
    "    for fold_num in range(5):\n",
    "        model_management_num = fold_num + fold_set_number*5\n",
    "        pd.DataFrame(test_pred_odds_prod[:, fold_num], index=X_test.index, columns=['pred'])\\\n",
    "            .to_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46951ca9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d13de4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawfile_dir = '../input/'\n",
    "dataset_dir = '../processed/dataset/'\n",
    "model_output_dir = f'../processed/lgb_output/'\n",
    "submitfile_dir = '../output/'\n",
    "if not os.path.isdir(submitfile_dir):\n",
    "    os.makedirs(submitfile_dir)\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "real_id = pd.read_pickle(os.path.join(dataset_dir, 'real_id.pickle'))\n",
    "fake_id = pd.read_pickle(os.path.join(dataset_dir, 'fake_id.pickle'))\n",
    "\n",
    "oof_pred_list = []\n",
    "test_pred_list = []\n",
    "for fold_set_number in range(10):\n",
    "    _oof_pred = pd.read_pickle(os.path.join(model_output_dir, f'oof_preds_{fold_set_number}.pkl.gz'))\\\n",
    "        .rank(pct=True).reset_index()\n",
    "    for model_management_num in range(fold_set_number, fold_set_number+5):\n",
    "        _test_pred = pd.read_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'))\n",
    "        _real_test_pred = _test_pred.loc[real_id]\n",
    "        _fake_test_pred = _test_pred.loc[fake_id]\n",
    "        _real_test_pred = _real_test_pred.rank(pct=True).reset_index()\n",
    "        _fake_test_pred = _fake_test_pred.rank(pct=True).reset_index()\n",
    "        test_pred_list.append(pd.concat([_real_test_pred, _fake_test_pred], axis=0))\n",
    "    _auc = roc_auc_score(y_train.loc[_oof_pred['ID_code']], _oof_pred[f'pred'])\n",
    "    print('{0} : {1:.6f}'.format(fold_set_number, _auc))\n",
    "    oof_pred_list.append(_oof_pred)\n",
    "oof_pred_concat_rank = pd.concat(oof_pred_list, axis=0)\n",
    "test_pred_concat_rank = pd.concat(test_pred_list, axis=0)\n",
    "oof_pred_rank_mean = oof_pred_concat_rank.groupby('ID_code').mean()\n",
    "test_pred_rank_mean = test_pred_concat_rank.groupby('ID_code').mean()\n",
    "total_auc = roc_auc_score(y_train.loc[oof_pred_rank_mean.index], oof_pred_rank_mean)\n",
    "print('total mean auc : {0:.6f}'.format(total_auc))\n",
    "\n",
    "sub = pd.read_csv(os.path.join(rawfile_dir, 'sample_submission.csv.zip'))\n",
    "sub = sub[['ID_code']].merge(\n",
    "    test_pred_rank_mean.reset_index().rename(columns={'pred':'target'}), how='left').fillna(0)\n",
    "sub.to_csv(os.path.join(submitfile_dir, 'akiyama_lgb.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669087e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf33ce",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476d4dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('CPU train: ' + cpuinfo.get_cpu_info()['brand_raw'] + ' ' + cpuinfo.get_cpu_info()['hz_advertised_friendly'])\n",
    "class RankGaussScalar(object):\n",
    "    \"\"\"\n",
    "    usage: \n",
    "    rgs = RankGaussScalar()\n",
    "    rgs.fit(df_X)\n",
    "    df_X_converted = rgs.transform(df_X)\n",
    "    df_X_test_converted = rgs.transform(df_X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.fit_done = False\n",
    "\n",
    "    def rank_gauss(self, x):\n",
    "        N = x.shape[0]\n",
    "        temp = x.argsort()\n",
    "        rank_x = temp.argsort() / N\n",
    "        rank_x -= rank_x.mean()\n",
    "        rank_x *= 2\n",
    "        efi_x = erfinv(rank_x)\n",
    "        efi_x -= efi_x.mean()\n",
    "        return efi_x\n",
    "\n",
    "    def fit(self, df_x):\n",
    "        \"\"\"\n",
    "        df_x: fitting対象のDataFrame\n",
    "        \"\"\"\n",
    "        self.train_unique_rankgauss = {}\n",
    "        self.target_cols = np.sort(df_x.columns)\n",
    "        for c in self.target_cols:\n",
    "            unique_val = np.sort(df_x[c].unique())\n",
    "            self.train_unique_rankgauss[c]= [unique_val, self.rank_gauss(unique_val)]\n",
    "        self.fit_done = True\n",
    "\n",
    "    def transform(self, df_target):\n",
    "        \"\"\"\n",
    "        df_target: transform対象のDataFrame\n",
    "        \"\"\"\n",
    "        assert self.fit_done\n",
    "        assert np.all(np.sort(np.intersect1d(df_target.columns, self.target_cols)) == np.sort(self.target_cols))\n",
    "        df_converted_rank_gauss = pd.DataFrame(index=df_target.index)\n",
    "        for c in self.target_cols:\n",
    "            df_converted_rank_gauss[c] = np.interp(df_target[c], \n",
    "                                                   self.train_unique_rankgauss[c][0], \n",
    "                                                   self.train_unique_rankgauss[c][1]) # ,left=0, right=0)\n",
    "        return df_converted_rank_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3544d0b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model_input = Input(shape=(6,))\n",
    "    num_input = Lambda(lambda x: x[:, :5], output_shape=(5,))(model_input)\n",
    "    cat_input = Lambda(lambda x: x[:, 5:], output_shape=(1,))(model_input)\n",
    "    \n",
    "    x = num_input\n",
    "    x = Dense(256)(num_input)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "\n",
    "    emb_dim = 128\n",
    "    y = Embedding(200, emb_dim, input_length=1)(cat_input)\n",
    "    y = Flatten()(y)\n",
    "    \n",
    "    z = Concatenate()([x, y])\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    output = Dense(1, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=model_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a91e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_output_dir = f'../processed/nn_output/'\n",
    "if not os.path.isdir(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "dataset_dir = '../processed/dataset/'\n",
    "X_train = pd.read_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "X_test = pd.read_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 1024\n",
    "patience = 5\n",
    "\n",
    "weights_dir = '../processed/keras_weights'\n",
    "if not os.path.isdir(weights_dir):\n",
    "    os.makedirs(weights_dir)\n",
    "\n",
    "for fold_set_number in range(10):\n",
    "    print('### start iter {} in 10 ###'.format(fold_set_number+1))\n",
    "    K.clear_session()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019+fold_set_number)\n",
    "    folds = [\n",
    "        [\n",
    "            np.concatenate([_trn+i * X_train.shape[0] for i in range(200)]), \n",
    "            np.concatenate([_val+i * X_train.shape[0] for i in range(200)])\n",
    "        ] for _trn, _val in skf.split(X_train, y_train)]\n",
    "\n",
    "    for fold_num in range(5):\n",
    "        print(f'## Start KFold number {fold_num} ##')\n",
    "        model_management_num = fold_num + fold_set_number*5\n",
    "        skf_train_index, skf_valid_index = folds[fold_num]\n",
    "\n",
    "        nonprogress_counter=0\n",
    "        e_auc_best = 0\n",
    "\n",
    "        weight_path = os.path.join(weights_dir, f'{model_management_num}.model')\n",
    "        skf_X_train = train_dset[0].iloc[skf_train_index].copy()\n",
    "        skf_y_train = train_dset[1].iloc[skf_train_index]\n",
    "        skf_X_valid = train_dset[0].iloc[skf_valid_index].copy()\n",
    "        skf_y_valid = train_dset[1].iloc[skf_valid_index]\n",
    "        single_valid_index = skf_valid_index[:skf_valid_index.shape[0]//200]  \n",
    "\n",
    "        rgscaler = RankGaussScalar()\n",
    "        rgscaler.fit(skf_X_train.iloc[:, :5].astype(float))\n",
    "        skf_X_train.iloc[:, :5] = rgscaler.transform(skf_X_train.iloc[:, :5].astype(float))\n",
    "        skf_X_valid.iloc[:, :5] = rgscaler.transform(skf_X_valid.iloc[:, :5].astype(float))\n",
    "\n",
    "        print('start training. ')\n",
    "        for _e in range(epochs):\n",
    "            print('epoch {}'.format(_e))\n",
    "            if _e == 0:\n",
    "                model = build_model()\n",
    "                #optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\n",
    "                optimizer = optimizers.adam(lr=0.001)\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "            history = model.fit(skf_X_train.values, skf_y_train,\n",
    "                            validation_data=[skf_X_valid.values, skf_y_valid], \n",
    "                            epochs=1,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            verbose=1)\n",
    "\n",
    "            oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "            for cnum in range(200):\n",
    "                oof_pred_array[:, cnum] = np.squeeze(\n",
    "                    model.predict(\n",
    "                        skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "                    )\n",
    "                )\n",
    "            e_auc = roc_auc_score(y_train.iloc[single_valid_index], oof_pred_array.prod(axis=1))\n",
    "\n",
    "            print('\\tauc : {0:.6f}'.format(e_auc))\n",
    "            if e_auc > e_auc_best:\n",
    "                model.save_weights(weight_path)\n",
    "                e_auc_best = e_auc\n",
    "                nonprogress_counter = 0\n",
    "            else:\n",
    "                nonprogress_counter += 1\n",
    "\n",
    "            if (nonprogress_counter >= patience) or (_e == (epochs-1)):\n",
    "                print('fold end. ')\n",
    "                break\n",
    "        print('training end. ')\n",
    "\n",
    "        model.load_weights(weight_path)\n",
    "\n",
    "        print('start predicting. ')\n",
    "        oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "        test_pred_array = np.ones((X_test.shape[0], 200))\n",
    "        for cnum in range(200):\n",
    "            tmp_X_test = arrange_dataset(X_test, cnum).copy()\n",
    "            tmp_X_test.iloc[:, :5] = rgscaler.transform(tmp_X_test.iloc[:, :5].astype(float))\n",
    "            oof_pred_array[:, cnum] = np.squeeze(\n",
    "                model.predict(\n",
    "                    skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "                )\n",
    "            )\n",
    "            test_pred_array[:, cnum] = np.squeeze(model.predict(tmp_X_test.values, batch_size=100000))\n",
    "        fold_oof_pred = pd.DataFrame(oof_pred_array, index=X_train.index[single_valid_index])\n",
    "        fold_test_pred = pd.DataFrame(test_pred_array, index=X_test.index)\n",
    "        print('prediction end. ')\n",
    "\n",
    "        print('save fold results')\n",
    "        fold_oof_pred.to_pickle(os.path.join(model_output_dir, f'oof_preds_{model_management_num}.pkl.gz'), compression='gzip')\n",
    "        fold_test_pred.to_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e6640",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70394a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawfile_dir = '../input/'\n",
    "dataset_dir = '../processed/dataset/'\n",
    "model_output_dir = f'../processed/nn_output/'\n",
    "submitfile_dir = '../output/'\n",
    "if not os.path.isdir(submitfile_dir):\n",
    "    os.makedirs(submitfile_dir)\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "real_id = pd.read_pickle(os.path.join(dataset_dir, 'real_id.pickle'))\n",
    "fake_id = pd.read_pickle(os.path.join(dataset_dir, 'fake_id.pickle'))\n",
    "\n",
    "oof_pred_list = []\n",
    "test_pred_list = []\n",
    "for model_management_num in range(50):\n",
    "    _oof_pred = pd.read_pickle(os.path.join(model_output_dir, f'oof_preds_{model_management_num}.pkl.gz'))\n",
    "    _oof_pred = (_oof_pred / (1 - _oof_pred)).prod(axis=1).rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _test_pred = pd.read_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'))\n",
    "    _test_pred = (_test_pred / (1 - _test_pred)).prod(axis=1)\n",
    "    _real_test_pred = _test_pred.loc[real_id]\n",
    "    _fake_test_pred = _test_pred.loc[fake_id]\n",
    "    _real_test_pred = _real_test_pred.rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _fake_test_pred = _fake_test_pred.rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _auc = roc_auc_score(y_train.loc[_oof_pred['ID_code']], _oof_pred[f'pred'])\n",
    "    print('{0} : {1:.6f}'.format(model_management_num, _auc))\n",
    "    oof_pred_list.append(_oof_pred)\n",
    "    test_pred_list.append(pd.concat([_real_test_pred, _fake_test_pred], axis=0))\n",
    "oof_pred_concat_rank = pd.concat(oof_pred_list, axis=0)\n",
    "test_pred_concat_rank = pd.concat(test_pred_list, axis=0)\n",
    "oof_pred_rank_mean = oof_pred_concat_rank.groupby('ID_code').mean()\n",
    "test_pred_rank_mean = test_pred_concat_rank.groupby('ID_code').mean()\n",
    "total_auc = roc_auc_score(y_train.loc[oof_pred_rank_mean.index], oof_pred_rank_mean)\n",
    "print('total mean auc : {0:.6f}'.format(total_auc))\n",
    "\n",
    "sub = pd.read_csv(os.path.join(rawfile_dir, 'sample_submission.csv.zip'))\n",
    "sub = sub[['ID_code']].merge(test_pred_rank_mean.reset_index()\\\n",
    "                             .rename(columns={'pred':'target'}), how='left').fillna(0)\n",
    "sub.to_csv(os.path.join(submitfile_dir, 'akiyama_nn.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95142d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7. Chuyển dự đoán từ dạng xác suất sang tỉ lệ (Continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcfb7c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
