{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "423bb0d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# KHDLUD Team 10\n",
    "\n",
    "Thành viên:<br>\n",
    "Trần Quốc Long - 18120202\n",
    "<br>Nguyễn Huy Danh - 1712318\n",
    "<br>Trần Đức Anh - 18120280\n",
    "<br>Du Chí Nhân - 18120492"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd808578",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9cc98f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cpuinfo\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, BatchNormalization,Dropout, Embedding, Flatten, Concatenate, Input\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "from scipy.special import erfinv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1830b23f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Đường dẫn\n",
    "test_original_data  = '../input/test.csv'\n",
    "train_original_data = '../input/train.csv'\n",
    "sample_submission_file = '../input/sample_submission.csv'\n",
    "SUBMIT_FILE_PATH = f'../output/2nd-place-solution.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f22828",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ddd97",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Link cuộc thi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e913e8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "kaggle competion:\n",
    "https://www.kaggle.com/c/santander-customer-transaction-prediction#\n",
    "    \n",
    "solution git:\n",
    "https://github.com/KazukiOnodera/Santander-Customer-Transaction-Prediction/blob/master/final_solution/akiyama/py/lgb_train_and_predict.py\n",
    "\n",
    "golf src:\n",
    "https://github.com/KazukiOnodera/santander-customer-transaction-prediction/blob/master/py/990_2nd_place_solution_golf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c90eadf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Giới thiệu chủ đề: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b29775",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tại Santander, sứ mệnh của chúng tôi là giúp mọi người và doanh nghiệp phát triển thịnh vượng. Chúng tôi luôn tìm cách giúp khách hàng hiểu được sức khỏe tài chính của họ và xác định những sản phẩm và dịch vụ nào có thể giúp họ đạt được các mục tiêu về tiền tệ của mình.\n",
    "\n",
    "Nhóm khoa học dữ liệu của chúng tôi liên tục thử thách các thuật toán học máy và làm việc với cộng đồng khoa học dữ liệu toàn cầu để đảm bảo chúng tôi có thể xác định chính xác hơn các cách mới để giải quyết thách thức phổ biến nhất, các vấn đề phân loại nhị phân, chẳng hạn như:\n",
    "\n",
    "Khách hàng có hài lòng không? \n",
    "Một khách hàng sẽ mua sản phẩm này? \n",
    "Khách hàng có thể trả khoản vay này không?\n",
    "\n",
    "Trong thử thách này, chúng tôi mời Kagglers giúp chúng tôi xác định khách hàng nào sẽ thực hiện một giao dịch cụ thể trong tương lai, bất kể số tiền đã giao dịch. Dữ liệu được cung cấp cho cuộc thi này có cấu trúc giống với dữ liệu thực mà chúng tôi có sẵn để giải quyết vấn đề này."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7e37b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tập dữ liệu huấn luyện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8d967f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>train_199995</td>\n",
       "      <td>0</td>\n",
       "      <td>11.4880</td>\n",
       "      <td>-0.4956</td>\n",
       "      <td>8.2622</td>\n",
       "      <td>3.5142</td>\n",
       "      <td>10.3404</td>\n",
       "      <td>11.6081</td>\n",
       "      <td>5.6709</td>\n",
       "      <td>15.1516</td>\n",
       "      <td>...</td>\n",
       "      <td>6.1415</td>\n",
       "      <td>13.2305</td>\n",
       "      <td>3.9901</td>\n",
       "      <td>0.9388</td>\n",
       "      <td>18.0249</td>\n",
       "      <td>-1.7939</td>\n",
       "      <td>2.1661</td>\n",
       "      <td>8.5326</td>\n",
       "      <td>16.6660</td>\n",
       "      <td>-17.8661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>train_199996</td>\n",
       "      <td>0</td>\n",
       "      <td>4.9149</td>\n",
       "      <td>-2.4484</td>\n",
       "      <td>16.7052</td>\n",
       "      <td>6.6345</td>\n",
       "      <td>8.3096</td>\n",
       "      <td>-10.5628</td>\n",
       "      <td>5.8802</td>\n",
       "      <td>21.5940</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9611</td>\n",
       "      <td>4.6549</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>1.8341</td>\n",
       "      <td>22.2717</td>\n",
       "      <td>1.7337</td>\n",
       "      <td>-2.1651</td>\n",
       "      <td>6.7419</td>\n",
       "      <td>15.9054</td>\n",
       "      <td>0.3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>train_199997</td>\n",
       "      <td>0</td>\n",
       "      <td>11.2232</td>\n",
       "      <td>-5.0518</td>\n",
       "      <td>10.5127</td>\n",
       "      <td>5.6456</td>\n",
       "      <td>9.3410</td>\n",
       "      <td>-5.4086</td>\n",
       "      <td>4.5555</td>\n",
       "      <td>21.5571</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0651</td>\n",
       "      <td>5.4414</td>\n",
       "      <td>3.1032</td>\n",
       "      <td>4.8793</td>\n",
       "      <td>23.5311</td>\n",
       "      <td>-1.5736</td>\n",
       "      <td>1.2832</td>\n",
       "      <td>8.7155</td>\n",
       "      <td>13.8329</td>\n",
       "      <td>4.1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>train_199998</td>\n",
       "      <td>0</td>\n",
       "      <td>9.7148</td>\n",
       "      <td>-8.6098</td>\n",
       "      <td>13.6104</td>\n",
       "      <td>5.7930</td>\n",
       "      <td>12.5173</td>\n",
       "      <td>0.5339</td>\n",
       "      <td>6.0479</td>\n",
       "      <td>17.0152</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6840</td>\n",
       "      <td>8.6587</td>\n",
       "      <td>2.7337</td>\n",
       "      <td>11.1178</td>\n",
       "      <td>20.4158</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>6.7980</td>\n",
       "      <td>10.0342</td>\n",
       "      <td>15.5289</td>\n",
       "      <td>-13.9001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>train_199999</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8762</td>\n",
       "      <td>-5.7105</td>\n",
       "      <td>12.1183</td>\n",
       "      <td>8.0328</td>\n",
       "      <td>11.5577</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>5.2839</td>\n",
       "      <td>15.2058</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9842</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.3766</td>\n",
       "      <td>15.2101</td>\n",
       "      <td>-2.4907</td>\n",
       "      <td>-2.2342</td>\n",
       "      <td>8.1857</td>\n",
       "      <td>12.1284</td>\n",
       "      <td>0.1385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID_code  target    var_0   var_1    var_2   var_3    var_4  \\\n",
       "0            train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607   \n",
       "1            train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622   \n",
       "2            train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825   \n",
       "3            train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846   \n",
       "4            train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772   \n",
       "...              ...     ...      ...     ...      ...     ...      ...   \n",
       "199995  train_199995       0  11.4880 -0.4956   8.2622  3.5142  10.3404   \n",
       "199996  train_199996       0   4.9149 -2.4484  16.7052  6.6345   8.3096   \n",
       "199997  train_199997       0  11.2232 -5.0518  10.5127  5.6456   9.3410   \n",
       "199998  train_199998       0   9.7148 -8.6098  13.6104  5.7930  12.5173   \n",
       "199999  train_199999       0  10.8762 -5.7105  12.1183  8.0328  11.5577   \n",
       "\n",
       "          var_5   var_6    var_7  ...  var_190  var_191  var_192  var_193  \\\n",
       "0       -9.2834  5.1187  18.6266  ...   4.4354   3.9642   3.1364   1.6910   \n",
       "1        7.0433  5.6208  16.5338  ...   7.6421   7.7214   2.5837  10.9516   \n",
       "2       -9.0837  6.9427  14.6155  ...   2.9057   9.7905   1.6704   1.6858   \n",
       "3       -1.8361  5.8428  14.9250  ...   4.4666   4.7433   0.7178   1.4214   \n",
       "4        2.4486  5.9405  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942   \n",
       "...         ...     ...      ...  ...      ...      ...      ...      ...   \n",
       "199995  11.6081  5.6709  15.1516  ...   6.1415  13.2305   3.9901   0.9388   \n",
       "199996 -10.5628  5.8802  21.5940  ...   4.9611   4.6549   0.6998   1.8341   \n",
       "199997  -5.4086  4.5555  21.5571  ...   4.0651   5.4414   3.1032   4.8793   \n",
       "199998   0.5339  6.0479  17.0152  ...   2.6840   8.6587   2.7337  11.1178   \n",
       "199999   0.3488  5.2839  15.2058  ...   8.9842   1.6893   0.1276   0.3766   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0       18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1       15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2       21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3       23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4       13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "199995  18.0249  -1.7939   2.1661   8.5326  16.6660 -17.8661  \n",
       "199996  22.2717   1.7337  -2.1651   6.7419  15.9054   0.3388  \n",
       "199997  23.5311  -1.5736   1.2832   8.7155  13.8329   4.1995  \n",
       "199998  20.4158  -0.0786   6.7980  10.0342  15.5289 -13.9001  \n",
       "199999  15.2101  -2.4907  -2.2342   8.1857  12.1284   0.1385  \n",
       "\n",
       "[200000 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_original_data)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af22f6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tập dữ liệu kiểm tra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9a0f8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>test_199995</td>\n",
       "      <td>13.1678</td>\n",
       "      <td>1.0136</td>\n",
       "      <td>10.4333</td>\n",
       "      <td>6.7997</td>\n",
       "      <td>8.5974</td>\n",
       "      <td>-4.1641</td>\n",
       "      <td>4.8579</td>\n",
       "      <td>14.7625</td>\n",
       "      <td>-2.7239</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>9.6849</td>\n",
       "      <td>4.6734</td>\n",
       "      <td>-1.3660</td>\n",
       "      <td>12.8721</td>\n",
       "      <td>1.2013</td>\n",
       "      <td>-4.6195</td>\n",
       "      <td>9.1568</td>\n",
       "      <td>18.2102</td>\n",
       "      <td>4.8801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>test_199996</td>\n",
       "      <td>9.7171</td>\n",
       "      <td>-9.1462</td>\n",
       "      <td>7.3443</td>\n",
       "      <td>9.1421</td>\n",
       "      <td>12.8936</td>\n",
       "      <td>3.0191</td>\n",
       "      <td>5.6888</td>\n",
       "      <td>18.8862</td>\n",
       "      <td>5.0915</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0071</td>\n",
       "      <td>6.6548</td>\n",
       "      <td>1.8197</td>\n",
       "      <td>2.4104</td>\n",
       "      <td>18.9037</td>\n",
       "      <td>-0.9337</td>\n",
       "      <td>2.9995</td>\n",
       "      <td>9.1112</td>\n",
       "      <td>18.1740</td>\n",
       "      <td>-20.7689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>test_199997</td>\n",
       "      <td>11.6360</td>\n",
       "      <td>2.2769</td>\n",
       "      <td>11.2074</td>\n",
       "      <td>7.7649</td>\n",
       "      <td>12.6796</td>\n",
       "      <td>11.3224</td>\n",
       "      <td>5.3883</td>\n",
       "      <td>18.3794</td>\n",
       "      <td>1.6603</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1536</td>\n",
       "      <td>2.6498</td>\n",
       "      <td>2.4937</td>\n",
       "      <td>-0.0637</td>\n",
       "      <td>20.0609</td>\n",
       "      <td>-1.1742</td>\n",
       "      <td>-4.1524</td>\n",
       "      <td>9.1933</td>\n",
       "      <td>11.7905</td>\n",
       "      <td>-22.2762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>test_199998</td>\n",
       "      <td>13.5745</td>\n",
       "      <td>-0.5134</td>\n",
       "      <td>13.6584</td>\n",
       "      <td>7.4855</td>\n",
       "      <td>11.2241</td>\n",
       "      <td>-11.3037</td>\n",
       "      <td>4.1959</td>\n",
       "      <td>16.8280</td>\n",
       "      <td>5.3208</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4259</td>\n",
       "      <td>8.5012</td>\n",
       "      <td>2.2713</td>\n",
       "      <td>5.7621</td>\n",
       "      <td>17.0056</td>\n",
       "      <td>1.1763</td>\n",
       "      <td>-2.3761</td>\n",
       "      <td>8.1079</td>\n",
       "      <td>8.7735</td>\n",
       "      <td>-0.2122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>test_199999</td>\n",
       "      <td>10.4664</td>\n",
       "      <td>1.8070</td>\n",
       "      <td>10.2277</td>\n",
       "      <td>6.0654</td>\n",
       "      <td>10.0258</td>\n",
       "      <td>1.0789</td>\n",
       "      <td>4.8879</td>\n",
       "      <td>14.4892</td>\n",
       "      <td>-0.5902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>9.2828</td>\n",
       "      <td>1.3601</td>\n",
       "      <td>4.8985</td>\n",
       "      <td>20.0926</td>\n",
       "      <td>-1.3048</td>\n",
       "      <td>-2.5981</td>\n",
       "      <td>10.3378</td>\n",
       "      <td>14.3340</td>\n",
       "      <td>-7.7094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID_code    var_0    var_1    var_2   var_3    var_4    var_5  \\\n",
       "0            test_0  11.0656   7.7798  12.9536  9.4292  11.4327  -2.3805   \n",
       "1            test_1   8.5304   1.2543  11.3047  5.1858   9.1974  -4.0117   \n",
       "2            test_2   5.4827 -10.3581  10.1407  7.0479  10.2628   9.8052   \n",
       "3            test_3   8.5374  -1.3222  12.0220  6.5749   8.8458   3.1744   \n",
       "4            test_4  11.7058  -0.1327  14.1295  7.7506   9.1035  -8.5848   \n",
       "...             ...      ...      ...      ...     ...      ...      ...   \n",
       "199995  test_199995  13.1678   1.0136  10.4333  6.7997   8.5974  -4.1641   \n",
       "199996  test_199996   9.7171  -9.1462   7.3443  9.1421  12.8936   3.0191   \n",
       "199997  test_199997  11.6360   2.2769  11.2074  7.7649  12.6796  11.3224   \n",
       "199998  test_199998  13.5745  -0.5134  13.6584  7.4855  11.2241 -11.3037   \n",
       "199999  test_199999  10.4664   1.8070  10.2277  6.0654  10.0258   1.0789   \n",
       "\n",
       "         var_6    var_7   var_8  ...  var_190  var_191  var_192  var_193  \\\n",
       "0       5.8493  18.2675  2.1337  ...  -2.1556  11.8495  -1.4300   2.4508   \n",
       "1       6.0196  18.6316 -4.4131  ...  10.6165   8.8349   0.9403  10.1282   \n",
       "2       4.8950  20.2537  1.5233  ...  -0.7484  10.9935   1.9803   2.1800   \n",
       "3       4.9397  20.5660  3.3755  ...   9.5702   9.0766   1.6580   3.5813   \n",
       "4       6.8595  10.6048  2.9890  ...   4.2259   9.1723   1.2835   3.3778   \n",
       "...        ...      ...     ...  ...      ...      ...      ...      ...   \n",
       "199995  4.8579  14.7625 -2.7239  ...   2.0544   9.6849   4.6734  -1.3660   \n",
       "199996  5.6888  18.8862  5.0915  ...   5.0071   6.6548   1.8197   2.4104   \n",
       "199997  5.3883  18.3794  1.6603  ...   5.1536   2.6498   2.4937  -0.0637   \n",
       "199998  4.1959  16.8280  5.3208  ...   3.4259   8.5012   2.2713   5.7621   \n",
       "199999  4.8879  14.4892 -0.5902  ...   0.1398   9.2828   1.3601   4.8985   \n",
       "\n",
       "        var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0       13.7112   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1       15.5765   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2       12.9813   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3       15.1874   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4       19.5542  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "...         ...      ...      ...      ...      ...      ...  \n",
       "199995  12.8721   1.2013  -4.6195   9.1568  18.2102   4.8801  \n",
       "199996  18.9037  -0.9337   2.9995   9.1112  18.1740 -20.7689  \n",
       "199997  20.0609  -1.1742  -4.1524   9.1933  11.7905 -22.2762  \n",
       "199998  17.0056   1.1763  -2.3761   8.1079   8.7735  -0.2122  \n",
       "199999  20.0926  -1.3048  -2.5981  10.3378  14.3340  -7.7094  \n",
       "\n",
       "[200000 rows x 201 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(test_original_data)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230ba1b7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cách tính điểm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da607342",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Kết quả sẽ được đánh giá dựa trên đối tượng đó có khả năng sẽ thanh toán trong tương lại hay không. Điểm số sẽ được đánh giá bằng độ đo khu vực dưới đường cong ROC giữa xác suất xảy ra và kết quả mục tiêu do sự bất cân đối giữa 2 giá trị 0 và 1 ( Không và có)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5f50ef",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    179902\n",
       "1     20098\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = train['target'].value_counts()\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d99e2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2. Các bước thực hiện"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad882a14",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 1. Loại bỏ những giá trị giả"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05715a0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Link: https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split\n",
    "\n",
    "Input: Dữ liệu của tập test\n",
    "\n",
    "Output: Dữ liệu của tập test đã được loại bỏ dữ liệu giả (Fake Samples)\n",
    "\n",
    "Mục đích: Sau khi kiểm tra tập test và tập train thì một điều bất cập đã được nhận ra là: Trong trong tập test có các giá trị độc nhất ở trên từng tính trạng khác với tập train. Tác giả đã đoán rằng người ta đã sử dụng kĩ thuật sampling distributions (phân phối mẫu) từ dữ liệu mẫu thật trong thống kê để tạo ra một số mẫu tổng hợp trong dữ liệu test. Và chúng ta sẽ lọc đi những mẫu dữ liệu giả ấy vì những mẫu giả ấy sẽ không đóng vai trò tính điểm trong tập test chính thức.\n",
    "\n",
    "Quy trình thực hiện: Dựa trên kiến thức về phân phối mẫu, tác giả đã nghĩ ra một cách để phân loại các dữ liệu giả đấy khi kiểm tra đó chính là<br>\n",
    "+Nếu 1 mẫu có ít nhất 1 tính trạng mang giá trị độc nhất thì đó sẽ là mẫu thật<br>\n",
    "+Nếu 1 mẫu sau khi kiểm tra toàn bộ đều không có giá trị độc nhất thì đó chính là mẫu giả.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc1e283",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def find_real_id(test):\n",
    "    test_array = test.drop(['ID_code'], axis=1).values\n",
    "\n",
    "    unique_count = np.zeros_like(test_array)\n",
    "    for feature in tqdm(range(test_array.shape[1])):\n",
    "        _, index_, count_ = np.unique(test_array[:, feature], return_counts=True, return_index=True)\n",
    "        unique_count[index_[count_ == 1], feature] += 1\n",
    "\n",
    "    # Samples which have unique values are real the others are fake\n",
    "    real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "    synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "    \n",
    "    real_id = test.loc[real_samples_indexes, 'ID_code'].values\n",
    "    fake_id = test.loc[synthetic_samples_indexes, 'ID_code'].values\n",
    "    \n",
    "    return real_id, fake_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5555851",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 2. Ghép tập train và tập test sau đó nghịch đảo một số tính trạng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c65889",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Link: https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals\n",
    "\n",
    "Input: Dữ liệu của tập test và tập train\n",
    "\n",
    "Output: Dữ liệu dùng để xử sau khi kết hợp và nghịch đảo\n",
    "\n",
    "Mục đích: Sau khi kiểm tra xác suất xảy ra bằng phân phối bayes chúng ta thấy được có 2 loại biểu đồ là biểu đồ có xác suất P lệch về bên trái và lệch về bên phải. Chúng ta sẽ nghịch đảo 1 loại biểu đồ lại để cho các điểm tương đồng trở nên rõ ràng hơn.\n",
    "\n",
    "Quy trình thực hiện: Chúng ta sẽ tìm ở trên tập biểu đồ những biểu đồ có xác suất lệch về bên phải và nghịch đảo chúng để xác suất của chúng đồng nhất về bên trái."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deab97b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before:\n",
    "    \n",
    "![title](img/var_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b100fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After\n",
    "\n",
    "![Title](img/var_after.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df96770",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reverse_and_scale(df):\n",
    "    '''\n",
    "    original author : jiweiliu\n",
    "    ref : https://www.kaggle.com/jiweiliu/fast-pdf-calculation-with-correlation-matrix\n",
    "    '''\n",
    "    \n",
    "    def reverse(df):\n",
    "        reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n",
    "                    22,24,25,26,27,41,29,\n",
    "                    32,35,37,40,48,49,47,\n",
    "                    55,51,52,53,60,61,62,103,65,66,67,69,\n",
    "                    70,71,74,78,79,\n",
    "                    82,84,89,90,91,94,95,96,97,99,\n",
    "                    105,106,110,111,112,118,119,125,128,\n",
    "                    130,133,134,135,137,138,\n",
    "                    140,144,145,147,151,155,157,159,\n",
    "                    161,162,163,164,167,168,\n",
    "                    170,171,173,175,176,179,\n",
    "                    180,181,184,185,187,189,\n",
    "                    190,191,195,196,199]\n",
    "        reverse_list = ['var_%d'%i for i in reverse_list]\n",
    "        for col in tqdm(reverse_list):\n",
    "            df[col] = df[col]*(-1)\n",
    "        return df\n",
    "\n",
    "    def scale(df):\n",
    "        for col in tqdm(df.columns):\n",
    "            if col.startswith('var_'):\n",
    "                mean,std = df[col].mean(),df[col].std()\n",
    "                df[col] = (df[col]-mean)/std\n",
    "        return df\n",
    "    \n",
    "    df = reverse(df)\n",
    "    df = scale(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cf113",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 3. Standrad Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e90e7f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Dữ liệu sau khi đã loại bỏ fake và reverse\n",
    "\n",
    "Output: Dữ liệu đã chuẩn hóa\n",
    "\n",
    "Mục đích: Sau khi đã loại bỏ các mẫu fake, chuẩn hóa dữ liệu sẽ giúp dễ dàng nhận ra các điểm dữ liệu bất thường. Ngoài ra, còn giúp tránh được những vấn đề do  sự khác biệt về độ đo, phục vụ cho việc unpivot ở bước 6\n",
    "\n",
    "Quy trình thực hiện: StandardScaler của sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b616819",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_round(val, digit=0):\n",
    "        p = 10 ** digit\n",
    "        return (val * p * 2 + 1) // 2 / p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40d24e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 4. Count round Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd24062",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Output của bước 3\n",
    "\n",
    "Output: Dữ liệu cũ + các cột đếm số lượng theo từng cột\n",
    "\n",
    "Mục đích: Feature engineering, giúp cải thiện kết quả\n",
    "\n",
    "Quy trình thực hiện:<br>\n",
    "Count encoding - Với mỗi cột, đếm số lượng giá trị và tạo ra cột mới bằng cách ánh xạ: giá trị -> số lượng<br>\n",
    "Count round encoding - Tương tự như 4 nhưng giá trị được làm tròn lần lượt là 1,2 và 3 chữ số thập phân rồi mới đếm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41be9f51",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def round_and_count(all_df, fake_id):\n",
    "    real_df = all_df.drop(fake_id, axis=0)\n",
    "    \n",
    "    feature_list = []\n",
    "    \n",
    "    _feature = all_df.copy()\n",
    "    for c in tqdm(real_df.columns):\n",
    "        count_dict = real_df[c].value_counts().to_dict()\n",
    "        _feature[c] = all_df[c].map(count_dict)\n",
    "    _feature = _feature.add_prefix('concat_count_')\n",
    "    feature_list.append(_feature)\n",
    "    \n",
    "    for digit in range(4, 1, -1):\n",
    "        _feature = all_df.copy()\n",
    "        for c in tqdm(all_df.columns):\n",
    "            all_rounded = my_round(all_df[c], digit)\n",
    "            real_rounded = my_round(real_df[c], digit)\n",
    "            count_dict = real_rounded.value_counts().to_dict()\n",
    "            _feature[c] = all_rounded.map(count_dict)\n",
    "        _feature = _feature.add_prefix(f'concat_count_round{digit}_')\n",
    "        feature_list.append(_feature)\n",
    "    \n",
    "    concat_feature = pd.concat(feature_list, axis=1)\n",
    "    \n",
    "    return concat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc305267",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 44.81it/s]\n",
      "100%|██████████| 101/101 [00:00<00:00, 539.05it/s]\n",
      "100%|██████████| 200/200 [00:01<00:00, 149.88it/s]\n",
      "100%|██████████| 200/200 [00:28<00:00,  6.91it/s]\n",
      "100%|██████████| 200/200 [00:23<00:00,  8.67it/s]\n",
      "100%|██████████| 200/200 [00:17<00:00, 11.35it/s]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = pd.read_csv(train_original_data)\n",
    "test = pd.read_csv(test_original_data)\n",
    "concat_df = pd.concat([train.drop('target', axis=1), test], axis=0, sort=False).set_index('ID_code')\n",
    "train_id = train['ID_code']\n",
    "test_id = test['ID_code']\n",
    "\n",
    "real_id, fake_id = find_real_id(test)\n",
    "scaled_df = reverse_and_scale(concat_df)\n",
    "count_df = round_and_count(scaled_df, fake_id)\n",
    "\n",
    "all_feature_df = pd.concat([scaled_df, count_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de8891",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 5. Unpivot tất cả các cột"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4b8d7cd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: Output của bước 5\n",
    "\n",
    "Output: DF 4.000.000 dòng (Ban đầu là 200.000 x 200 nhưng, họ cho rằng giá trị các cột là độc lập với nhau và có thể dùng giá trị của 1 cột để dự đoán target nên họ unpivot)\n",
    "\n",
    "Mục đích: Hỗ trợ dự đoán target vì họ đã thử và thấy có cải thiện kết quả\n",
    "\n",
    "Thực hiện: Cứ mỗi biến sẽ là 7 cột (ID_code, raw_value, 4 cột đã encode, thuộc về cột nào) - shape (200.000 x 7) đem đi stack vào nhau thành 4.000.000 dòng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da4f7609",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def arrange_dataset(df, cnum):\n",
    "    _dset = df.filter(regex=f'var_{cnum}$')\n",
    "    _dset.columns = list(range(_dset.shape[1]))\n",
    "    _dset = _dset.assign(var_num = cnum)\n",
    "    return _dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da799f2f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dset_list = []\n",
    "for cnum in range(200):\n",
    "    _dset = arrange_dataset(all_feature_df.loc[train_id], cnum)\n",
    "    dset_list.append(_dset)\n",
    "\n",
    "concat_X_train = pd.concat(dset_list, axis=0)\n",
    "concat_X_train['var_num'] = concat_X_train['var_num'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20d0552",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Và đây là kết quả sau khi tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "545bf2e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>var_num</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>0.573929</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>114</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>-0.273590</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>0.677997</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>-0.128711</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>92</td>\n",
       "      <td>1051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>0.273969</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>139</td>\n",
       "      <td>1188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_199995</th>\n",
       "      <td>1.399417</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>542</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_199996</th>\n",
       "      <td>-0.347966</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>998</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_199997</th>\n",
       "      <td>-0.718532</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>122</td>\n",
       "      <td>1089</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_199998</th>\n",
       "      <td>1.018743</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>72</td>\n",
       "      <td>818</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_199999</th>\n",
       "      <td>-0.328740</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>967</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0  1   2    3     4 var_num\n",
       "ID_code                                         \n",
       "train_0       0.573929  8  17  114  1096       0\n",
       "train_1      -0.273590  3   6   90  1001       0\n",
       "train_2       0.677997  6  13  108  1028       0\n",
       "train_3      -0.128711  3   6   92  1051       0\n",
       "train_4       0.273969  8  18  139  1188       0\n",
       "...                ... ..  ..  ...   ...     ...\n",
       "train_199995  1.399417  2   5   39   542     199\n",
       "train_199996 -0.347966  2  11  100   998     199\n",
       "train_199997 -0.718532  1  16  122  1089     199\n",
       "train_199998  1.018743  1   6   72   818     199\n",
       "train_199999 -0.328740  1   8   85   967     199\n",
       "\n",
       "[40000000 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a91c351",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../processed/dataset/'\n",
    "if not os.path.isdir(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "all_feature_df.loc[train_id].to_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "all_feature_df.loc[test_id].to_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "train.set_index('ID_code').loc[train_id, 'target'].to_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "pd.to_pickle(real_id, os.path.join(dataset_dir, 'real_id.pickle'))\n",
    "pd.to_pickle(fake_id, os.path.join(dataset_dir, 'fake_id.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973bdbcc",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6. Training và Predict (Continue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f812845b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Input: dữ liệu sau bước unpivot được gán tên các cột vào\n",
    "\n",
    "Output: các model sau khi được train\n",
    "\n",
    "Mục đích: từ data sau khi đã chuẩn hóa, tạo ra các model, dùng các model này để dự đoán\n",
    "\n",
    "Mô tả: Trong solution, tác giả train và predict sử dụng 2 phương pháp, là Neural Network với thư viện keras và Gradient Boosting với framework LightBGM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa27998e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dbc219",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51a556c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ModelExtractionCallback(object):\n",
    "    \"\"\"\n",
    "    original author : momijiame\n",
    "    ref : https://blog.amedama.jp/entry/lightgbm-cv-model\n",
    "    description : Class for callback to extract trained models from lightgbm.cv(). \n",
    "    note: This class depends on private class '_CVBooster', so there are some future risks. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._model = None\n",
    "\n",
    "    def __call__(self, env):\n",
    "        # _CVBooster の参照を保持する\n",
    "        self._model = env.model\n",
    "\n",
    "    def _assert_called_cb(self):\n",
    "        if self._model is None:\n",
    "            # コールバックが呼ばれていないときは例外にする\n",
    "            raise RuntimeError('callback has not called yet')\n",
    "\n",
    "    @property\n",
    "    def boosters_proxy(self):\n",
    "        self._assert_called_cb()\n",
    "        # Booster へのプロキシオブジェクトを返す\n",
    "        return self._model\n",
    "\n",
    "    @property\n",
    "    def raw_boosters(self):\n",
    "        self._assert_called_cb()\n",
    "        # Booster のリストを返す\n",
    "        return self._model.boosters\n",
    "\n",
    "    @property\n",
    "    def best_iteration(self):\n",
    "        self._assert_called_cb()\n",
    "        # Early stop したときの boosting round を返す\n",
    "        return self._model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9487111a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_output_dir = f'../processed/lgb_output/'\n",
    "if not os.path.isdir(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "dataset_dir = '../processed/dataset/'\n",
    "X_train = pd.read_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "X_test = pd.read_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "\n",
    "params = {\n",
    "    'bagging_freq': 5,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 1.0,\n",
    "    'learning_rate': 0.005,\n",
    "    'max_depth': -1,\n",
    "    'metric':'binary_logloss',\n",
    "    'min_data_in_leaf': 30,\n",
    "    'min_sum_hessian_in_leaf': 10.0,\n",
    "    'num_leaves': 64,\n",
    "    'num_threads': cpu_count(),\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'verbosity': 1}\n",
    "\n",
    "train_dset = lgb.Dataset(\n",
    "    concat_X_train, \n",
    "    pd.concat([y_train for c in range(200)], axis=0), \n",
    "    free_raw_data=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac7ccb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('CPU train: ' + cpuinfo.get_cpu_info()['brand_raw'] + ' ' + cpuinfo.get_cpu_info()['hz_advertised_friendly'])\n",
    "for fold_set_number in range(10):\n",
    "    print('### start iter {} in 10 ###'.format(fold_set_number+1))\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019+fold_set_number)\n",
    "    folds = [\n",
    "        [\n",
    "            np.concatenate([_trn+i * X_train.shape[0] for i in range(200)]), \n",
    "            np.concatenate([_val+i * X_train.shape[0] for i in range(200)])\n",
    "        ] for _trn, _val in skf.split(X_train, y_train)]\n",
    "\n",
    "    extraction_cb = ModelExtractionCallback()\n",
    "    callbacks = [extraction_cb,]\n",
    "\n",
    "    print('start training. ')\n",
    "    cv_result = lgb.cv(params, train_set=train_dset, num_boost_round=100000, \n",
    "                             early_stopping_rounds=100, verbose_eval=100, folds=folds, callbacks=callbacks)\n",
    "    bsts = extraction_cb.raw_boosters\n",
    "    best_iteration = extraction_cb.best_iteration\n",
    "    print('training end. ')\n",
    "\n",
    "    print('start predicting. ')\n",
    "    oof_pred_array = np.ones((X_train.shape[0], 200))\n",
    "    test_pred_array = np.ones((X_test.shape[0], 5, 200))\n",
    "    for cnum in tqdm(range(200)):\n",
    "        for i, bst in enumerate(bsts):\n",
    "            cv_valid_index = bst.valid_sets[0].used_indices\n",
    "            cv_valid_index = cv_valid_index[:int(cv_valid_index.shape[0]/200)]\n",
    "            # oofの予測\n",
    "            cv_valid_data = arrange_dataset(X_train, cnum).iloc[cv_valid_index].values\n",
    "            oof_pred_array[cv_valid_index, cnum] = bst.predict(cv_valid_data, num_iteration=best_iteration)\n",
    "            # testの予測\n",
    "            test_pred_array[:, i, cnum] = bst.predict(arrange_dataset(X_test, cnum).values, num_iteration=best_iteration)\n",
    "    print('prediction end. ')\n",
    "\n",
    "    print('start postprocess. ')\n",
    "    thr = 0.500\n",
    "    oof_pred_odds_prod = np.ones((X_train.shape[0]))\n",
    "    test_pred_odds_prod = np.ones((X_test.shape[0], 5))\n",
    "    for cnum in tqdm(range(200)):\n",
    "        tmp_auc = roc_auc_score(y_train, oof_pred_array[:, cnum])\n",
    "        if tmp_auc >= thr:\n",
    "            oof_pred_odds_prod *= oof_pred_array[:, cnum] / (1 - oof_pred_array[:, cnum])\n",
    "            test_pred_odds_prod *= test_pred_array[:,:, cnum] / (1 - test_pred_array[:,:, cnum])\n",
    "    print('postprocess end. auc : {0:.6f}'.format(roc_auc_score(y_train, oof_pred_odds_prod)))\n",
    "\n",
    "    print('save iteration results')\n",
    "    pd.DataFrame(oof_pred_odds_prod, index=X_train.index, columns=['pred'])\\\n",
    "        .to_pickle(os.path.join(model_output_dir, f'oof_preds_{fold_set_number}.pkl.gz'), compression='gzip')\n",
    "    for fold_num in range(5):\n",
    "        model_management_num = fold_num + fold_set_number*5\n",
    "        pd.DataFrame(test_pred_odds_prod[:, fold_num], index=X_test.index, columns=['pred'])\\\n",
    "            .to_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46951ca9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d13de4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawfile_dir = '../input/'\n",
    "dataset_dir = '../processed/dataset/'\n",
    "model_output_dir = f'../processed/lgb_output/'\n",
    "submitfile_dir = '../output/'\n",
    "if not os.path.isdir(submitfile_dir):\n",
    "    os.makedirs(submitfile_dir)\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "real_id = pd.read_pickle(os.path.join(dataset_dir, 'real_id.pickle'))\n",
    "fake_id = pd.read_pickle(os.path.join(dataset_dir, 'fake_id.pickle'))\n",
    "\n",
    "oof_pred_list = []\n",
    "test_pred_list = []\n",
    "for fold_set_number in range(10):\n",
    "    _oof_pred = pd.read_pickle(os.path.join(model_output_dir, f'oof_preds_{fold_set_number}.pkl.gz'))\\\n",
    "        .rank(pct=True).reset_index()\n",
    "    for model_management_num in range(fold_set_number, fold_set_number+5):\n",
    "        _test_pred = pd.read_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'))\n",
    "        _real_test_pred = _test_pred.loc[real_id]\n",
    "        _fake_test_pred = _test_pred.loc[fake_id]\n",
    "        _real_test_pred = _real_test_pred.rank(pct=True).reset_index()\n",
    "        _fake_test_pred = _fake_test_pred.rank(pct=True).reset_index()\n",
    "        test_pred_list.append(pd.concat([_real_test_pred, _fake_test_pred], axis=0))\n",
    "    _auc = roc_auc_score(y_train.loc[_oof_pred['ID_code']], _oof_pred[f'pred'])\n",
    "    print('{0} : {1:.6f}'.format(fold_set_number, _auc))\n",
    "    oof_pred_list.append(_oof_pred)\n",
    "oof_pred_concat_rank = pd.concat(oof_pred_list, axis=0)\n",
    "test_pred_concat_rank = pd.concat(test_pred_list, axis=0)\n",
    "oof_pred_rank_mean = oof_pred_concat_rank.groupby('ID_code').mean()\n",
    "test_pred_rank_mean = test_pred_concat_rank.groupby('ID_code').mean()\n",
    "total_auc = roc_auc_score(y_train.loc[oof_pred_rank_mean.index], oof_pred_rank_mean)\n",
    "print('total mean auc : {0:.6f}'.format(total_auc))\n",
    "\n",
    "sub = pd.read_csv(os.path.join(rawfile_dir, 'sample_submission.csv.zip'))\n",
    "sub = sub[['ID_code']].merge(\n",
    "    test_pred_rank_mean.reset_index().rename(columns={'pred':'target'}), how='left').fillna(0)\n",
    "sub.to_csv(os.path.join(submitfile_dir, 'akiyama_lgb.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4669087e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf33ce",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476d4dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('CPU train: ' + cpuinfo.get_cpu_info()['brand_raw'] + ' ' + cpuinfo.get_cpu_info()['hz_advertised_friendly'])\n",
    "class RankGaussScalar(object):\n",
    "    \"\"\"\n",
    "    usage: \n",
    "    rgs = RankGaussScalar()\n",
    "    rgs.fit(df_X)\n",
    "    df_X_converted = rgs.transform(df_X)\n",
    "    df_X_test_converted = rgs.transform(df_X_test)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.fit_done = False\n",
    "\n",
    "    def rank_gauss(self, x):\n",
    "        N = x.shape[0]\n",
    "        temp = x.argsort()\n",
    "        rank_x = temp.argsort() / N\n",
    "        rank_x -= rank_x.mean()\n",
    "        rank_x *= 2\n",
    "        efi_x = erfinv(rank_x)\n",
    "        efi_x -= efi_x.mean()\n",
    "        return efi_x\n",
    "\n",
    "    def fit(self, df_x):\n",
    "        \"\"\"\n",
    "        df_x: fitting対象のDataFrame\n",
    "        \"\"\"\n",
    "        self.train_unique_rankgauss = {}\n",
    "        self.target_cols = np.sort(df_x.columns)\n",
    "        for c in self.target_cols:\n",
    "            unique_val = np.sort(df_x[c].unique())\n",
    "            self.train_unique_rankgauss[c]= [unique_val, self.rank_gauss(unique_val)]\n",
    "        self.fit_done = True\n",
    "\n",
    "    def transform(self, df_target):\n",
    "        \"\"\"\n",
    "        df_target: transform対象のDataFrame\n",
    "        \"\"\"\n",
    "        assert self.fit_done\n",
    "        assert np.all(np.sort(np.intersect1d(df_target.columns, self.target_cols)) == np.sort(self.target_cols))\n",
    "        df_converted_rank_gauss = pd.DataFrame(index=df_target.index)\n",
    "        for c in self.target_cols:\n",
    "            df_converted_rank_gauss[c] = np.interp(df_target[c], \n",
    "                                                   self.train_unique_rankgauss[c][0], \n",
    "                                                   self.train_unique_rankgauss[c][1]) # ,left=0, right=0)\n",
    "        return df_converted_rank_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3544d0b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model_input = Input(shape=(6,))\n",
    "    num_input = Lambda(lambda x: x[:, :5], output_shape=(5,))(model_input)\n",
    "    cat_input = Lambda(lambda x: x[:, 5:], output_shape=(1,))(model_input)\n",
    "    \n",
    "    x = num_input\n",
    "    x = Dense(256)(num_input)\n",
    "    x = PReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "\n",
    "    emb_dim = 128\n",
    "    y = Embedding(200, emb_dim, input_length=1)(cat_input)\n",
    "    y = Flatten()(y)\n",
    "    \n",
    "    z = Concatenate()([x, y])\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Dropout(rate=0.5)(z)\n",
    "    z = Dense(256)(z)\n",
    "    z = PReLU()(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    output = Dense(1, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=model_input, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a91e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_output_dir = f'../processed/nn_output/'\n",
    "if not os.path.isdir(model_output_dir):\n",
    "    os.makedirs(model_output_dir)\n",
    "\n",
    "dataset_dir = '../processed/dataset/'\n",
    "X_train = pd.read_pickle(os.path.join(dataset_dir, 'X_train.pickle'))\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "X_test = pd.read_pickle(os.path.join(dataset_dir, 'X_test.pickle'))\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 1024\n",
    "patience = 5\n",
    "\n",
    "weights_dir = '../processed/keras_weights'\n",
    "if not os.path.isdir(weights_dir):\n",
    "    os.makedirs(weights_dir)\n",
    "\n",
    "for fold_set_number in range(10):\n",
    "    print('### start iter {} in 10 ###'.format(fold_set_number+1))\n",
    "    K.clear_session()\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019+fold_set_number)\n",
    "    folds = [\n",
    "        [\n",
    "            np.concatenate([_trn+i * X_train.shape[0] for i in range(200)]), \n",
    "            np.concatenate([_val+i * X_train.shape[0] for i in range(200)])\n",
    "        ] for _trn, _val in skf.split(X_train, y_train)]\n",
    "\n",
    "    for fold_num in range(5):\n",
    "        print(f'## Start KFold number {fold_num} ##')\n",
    "        model_management_num = fold_num + fold_set_number*5\n",
    "        skf_train_index, skf_valid_index = folds[fold_num]\n",
    "\n",
    "        nonprogress_counter=0\n",
    "        e_auc_best = 0\n",
    "\n",
    "        weight_path = os.path.join(weights_dir, f'{model_management_num}.model')\n",
    "        skf_X_train = train_dset[0].iloc[skf_train_index].copy()\n",
    "        skf_y_train = train_dset[1].iloc[skf_train_index]\n",
    "        skf_X_valid = train_dset[0].iloc[skf_valid_index].copy()\n",
    "        skf_y_valid = train_dset[1].iloc[skf_valid_index]\n",
    "        single_valid_index = skf_valid_index[:skf_valid_index.shape[0]//200]  \n",
    "\n",
    "        rgscaler = RankGaussScalar()\n",
    "        rgscaler.fit(skf_X_train.iloc[:, :5].astype(float))\n",
    "        skf_X_train.iloc[:, :5] = rgscaler.transform(skf_X_train.iloc[:, :5].astype(float))\n",
    "        skf_X_valid.iloc[:, :5] = rgscaler.transform(skf_X_valid.iloc[:, :5].astype(float))\n",
    "\n",
    "        print('start training. ')\n",
    "        for _e in range(epochs):\n",
    "            print('epoch {}'.format(_e))\n",
    "            if _e == 0:\n",
    "                model = build_model()\n",
    "                #optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4)\n",
    "                optimizer = optimizers.adam(lr=0.001)\n",
    "                model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "            history = model.fit(skf_X_train.values, skf_y_train,\n",
    "                            validation_data=[skf_X_valid.values, skf_y_valid], \n",
    "                            epochs=1,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            verbose=1)\n",
    "\n",
    "            oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "            for cnum in range(200):\n",
    "                oof_pred_array[:, cnum] = np.squeeze(\n",
    "                    model.predict(\n",
    "                        skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "                    )\n",
    "                )\n",
    "            e_auc = roc_auc_score(y_train.iloc[single_valid_index], oof_pred_array.prod(axis=1))\n",
    "\n",
    "            print('\\tauc : {0:.6f}'.format(e_auc))\n",
    "            if e_auc > e_auc_best:\n",
    "                model.save_weights(weight_path)\n",
    "                e_auc_best = e_auc\n",
    "                nonprogress_counter = 0\n",
    "            else:\n",
    "                nonprogress_counter += 1\n",
    "\n",
    "            if (nonprogress_counter >= patience) or (_e == (epochs-1)):\n",
    "                print('fold end. ')\n",
    "                break\n",
    "        print('training end. ')\n",
    "\n",
    "        model.load_weights(weight_path)\n",
    "\n",
    "        print('start predicting. ')\n",
    "        oof_pred_array = np.ones((single_valid_index.shape[0], 200))\n",
    "        test_pred_array = np.ones((X_test.shape[0], 200))\n",
    "        for cnum in range(200):\n",
    "            tmp_X_test = arrange_dataset(X_test, cnum).copy()\n",
    "            tmp_X_test.iloc[:, :5] = rgscaler.transform(tmp_X_test.iloc[:, :5].astype(float))\n",
    "            oof_pred_array[:, cnum] = np.squeeze(\n",
    "                model.predict(\n",
    "                    skf_X_valid.iloc[cnum*single_valid_index.shape[0]:(cnum+1)*single_valid_index.shape[0]].values, batch_size=100000\n",
    "                )\n",
    "            )\n",
    "            test_pred_array[:, cnum] = np.squeeze(model.predict(tmp_X_test.values, batch_size=100000))\n",
    "        fold_oof_pred = pd.DataFrame(oof_pred_array, index=X_train.index[single_valid_index])\n",
    "        fold_test_pred = pd.DataFrame(test_pred_array, index=X_test.index)\n",
    "        print('prediction end. ')\n",
    "\n",
    "        print('save fold results')\n",
    "        fold_oof_pred.to_pickle(os.path.join(model_output_dir, f'oof_preds_{model_management_num}.pkl.gz'), compression='gzip')\n",
    "        fold_test_pred.to_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'), compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14e6640",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70394a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rawfile_dir = '../input/'\n",
    "dataset_dir = '../processed/dataset/'\n",
    "model_output_dir = f'../processed/nn_output/'\n",
    "submitfile_dir = '../output/'\n",
    "if not os.path.isdir(submitfile_dir):\n",
    "    os.makedirs(submitfile_dir)\n",
    "\n",
    "y_train = pd.read_pickle(os.path.join(dataset_dir, 'y_train.pickle'))\n",
    "real_id = pd.read_pickle(os.path.join(dataset_dir, 'real_id.pickle'))\n",
    "fake_id = pd.read_pickle(os.path.join(dataset_dir, 'fake_id.pickle'))\n",
    "\n",
    "oof_pred_list = []\n",
    "test_pred_list = []\n",
    "for model_management_num in range(50):\n",
    "    _oof_pred = pd.read_pickle(os.path.join(model_output_dir, f'oof_preds_{model_management_num}.pkl.gz'))\n",
    "    _oof_pred = (_oof_pred / (1 - _oof_pred)).prod(axis=1).rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _test_pred = pd.read_pickle(os.path.join(model_output_dir, f'test_preds_{model_management_num}.pkl.gz'))\n",
    "    _test_pred = (_test_pred / (1 - _test_pred)).prod(axis=1)\n",
    "    _real_test_pred = _test_pred.loc[real_id]\n",
    "    _fake_test_pred = _test_pred.loc[fake_id]\n",
    "    _real_test_pred = _real_test_pred.rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _fake_test_pred = _fake_test_pred.rank(pct=True).to_frame(name=f'pred').reset_index()\n",
    "    _auc = roc_auc_score(y_train.loc[_oof_pred['ID_code']], _oof_pred[f'pred'])\n",
    "    print('{0} : {1:.6f}'.format(model_management_num, _auc))\n",
    "    oof_pred_list.append(_oof_pred)\n",
    "    test_pred_list.append(pd.concat([_real_test_pred, _fake_test_pred], axis=0))\n",
    "oof_pred_concat_rank = pd.concat(oof_pred_list, axis=0)\n",
    "test_pred_concat_rank = pd.concat(test_pred_list, axis=0)\n",
    "oof_pred_rank_mean = oof_pred_concat_rank.groupby('ID_code').mean()\n",
    "test_pred_rank_mean = test_pred_concat_rank.groupby('ID_code').mean()\n",
    "total_auc = roc_auc_score(y_train.loc[oof_pred_rank_mean.index], oof_pred_rank_mean)\n",
    "print('total mean auc : {0:.6f}'.format(total_auc))\n",
    "\n",
    "sub = pd.read_csv(os.path.join(rawfile_dir, 'sample_submission.csv.zip'))\n",
    "sub = sub[['ID_code']].merge(test_pred_rank_mean.reset_index()\\\n",
    "                             .rename(columns={'pred':'target'}), how='left').fillna(0)\n",
    "sub.to_csv(os.path.join(submitfile_dir, 'akiyama_nn.csv.gz'), index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95142d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 7. Chuyển dự đoán từ dạng xác suất sang tỉ lệ (Continue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcfb7c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
